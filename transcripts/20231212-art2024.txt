1
00:02:30,000 --> 00:02:40,160
Bienvenue sur Radio Shad, on est le 12 décembre 2023 et puis c'est toujours la dernière

2
00:02:40,160 --> 00:02:44,640
semaine de l'année pour Radio Shad et c'est en même temps aussi probablement la dernière

3
00:02:44,640 --> 00:02:49,320
émission, ça dépendra un petit peu de demain, mais les chances sont élevées pour que ce

4
00:02:49,320 --> 00:02:53,720
soit la dernière émission de l'année aussi concernant l'intelligence artificielle et

5
00:02:53,720 --> 00:02:57,720
sur l'art, et donc d'autant plus sur les deux en même temps.

6
00:02:58,440 --> 00:03:08,600
Ce soir, du coup, on va parler de ça et pour ce faire, j'ai invité Carla Marand qui est

7
00:03:08,600 --> 00:03:10,600
une étudiante en…

28
00:04:26,080 --> 00:04:33,520
Je disais, je suis doctorante et j'écris ma thèse sur le sujet de l'art et de l'intelligence artificielle.

30
00:04:36,760 --> 00:04:39,760
Mais il me semble que tu avais deux formations.

32
00:04:40,760 --> 00:04:43,760
Ce n'est pas une formation.

33
00:04:43,760 --> 00:04:53,760
Le deuxième aspect, le premier aspect, le travail de thèse de doctorat, on appelle ça une formation diplomante.

34
00:04:53,760 --> 00:04:56,760
Mais en soi, c'est déjà un travail de recherche.

35
00:04:56,760 --> 00:04:59,760
On écrit sa thèse de son côté.

36
00:04:59,760 --> 00:05:01,760
Après, on est plus ou moins dirigé.

37
00:05:02,000 --> 00:05:05,000
C'est plus un rôle de conseil en soi.

38
00:05:05,000 --> 00:05:07,000
On écrit vraiment son travail.

39
00:05:07,000 --> 00:05:09,000
C'est son premier travail de chercheur.

40
00:05:09,000 --> 00:05:16,000
Et à côté, je travaille pour un projet de recherche national.

41
00:05:16,000 --> 00:05:20,000
C'est un projet de recherche qui est financé par l'Agence Nationale de la Recherche

42
00:05:20,000 --> 00:05:25,000
sur ce qu'on appelle la culture de l'intelligence artificielle.

43
00:05:25,000 --> 00:05:28,000
C'est sur l'approche culturelle de l'IA.

44
00:05:28,240 --> 00:05:33,240
On travaille plus bien sur ces conditions de développement, de création,

45
00:05:33,240 --> 00:05:36,240
le cadre culturel dans lequel elle est développée

46
00:05:36,240 --> 00:05:42,240
que dans les façons qu'on a de la représenter culturellement.

47
00:05:42,240 --> 00:05:53,240
C'est un travail à côté de gestion à la fois scientifique, du projet, administrative, financière, etc.

48
00:05:53,480 --> 00:05:57,480
On est une soixantaine de chercheurs à travailler sur le sujet.

49
00:05:57,480 --> 00:06:02,480
Je vous en parlerai plus longuement si vous le souhaitez.

50
00:06:04,480 --> 00:06:07,480
On va en reparler.

51
00:06:07,480 --> 00:06:09,480
En tout cas, très content d'être recevoir.

52
00:06:09,480 --> 00:06:16,480
C'est vrai qu'en général, on n'a pas d'invité spécifiquement pour la journée IA.

53
00:06:16,720 --> 00:06:21,720
Tous les mardis, on fait des débats sur l'intelligence artificielle,

54
00:06:21,720 --> 00:06:25,720
sur son impact dans nos vies, dans nos sociétés, etc.

55
00:06:25,720 --> 00:06:32,720
C'est bien parce que ça va faire un petit peu de fraîcheur dans nos discussions.

56
00:06:32,720 --> 00:06:38,720
On est toujours entre nous et on finit par revenir sur les mêmes trucs.

57
00:06:38,720 --> 00:06:41,720
On va tous mourir de toute façon.

58
00:06:41,960 --> 00:06:43,960
Parfois, c'est aussi ça qu'on étudie.

59
00:06:43,960 --> 00:06:46,960
C'est aussi le catastrophisme.

60
00:06:46,960 --> 00:06:51,960
Après, ce n'est jamais le rôle des chercheurs de dire ce qui va se passer,

61
00:06:51,960 --> 00:06:54,960
ce qui est bien et ce qui n'est pas bien.

62
00:06:54,960 --> 00:06:58,960
Les chercheurs ne posent jamais la morale.

63
00:06:58,960 --> 00:07:01,960
Ce n'est pas notre métier.

64
00:07:01,960 --> 00:07:14,960
Mais on va jeter la lumière sur les façons qu'on va avoir d'aborder l'intelligence artificielle,

65
00:07:14,960 --> 00:07:20,960
que ce soit en étant particulièrement enthousiaste ou en étant particulièrement alarmiste,

66
00:07:20,960 --> 00:07:26,960
pour voir comment on en arrive à cette conclusion sur l'IA et ce que ça dit de la société.

67
00:07:26,960 --> 00:07:30,960
Ce n'est pas tellement que s'intéresser à l'IA pour s'intéresser à l'IA.

68
00:07:30,960 --> 00:07:34,960
C'est aussi utiliser l'intelligence artificielle comme une lunette

69
00:07:34,960 --> 00:07:38,960
pour comprendre d'autres ressorts de la société.

70
00:07:40,960 --> 00:07:41,960
Ok.

71
00:07:41,960 --> 00:07:42,960
Alors, c'est sûr qu'on va revenir là-dessus.

72
00:07:42,960 --> 00:07:48,960
Mais un petit peu avant ça, j'ai envie de faire la revue des outils qui sont sortis cette semaine.

73
00:07:48,960 --> 00:07:54,960
Est-ce que tu regardes un peu l'actualité des différents technos, des différents outils?

74
00:07:54,960 --> 00:08:00,960
Que ce soit tous les trucs, par exemple, que ce soit texte ou vidéo, texte ou image, texte ou texte, etc.

75
00:08:00,960 --> 00:08:06,960
Alors oui, je me renseigne parce que je dois mener une veille, notamment sur le plan culturel.

76
00:08:06,960 --> 00:08:08,960
Mais je serais ravie d'en savoir plus.

77
00:08:10,960 --> 00:08:13,960
Alors, il y a beaucoup de développement en ce moment.

78
00:08:13,960 --> 00:08:15,960
C'est carrément fou.

79
00:08:15,960 --> 00:08:22,960
La semaine dernière, la semaine d'avant, j'avais parlé un petit peu de la technologie LCM.

80
00:08:22,960 --> 00:08:24,960
Si je ne dis pas de conneries, oui, c'est ça.

81
00:08:24,960 --> 00:08:27,960
C'est ce qui permet d'économiser du temps pour faire du...

82
00:08:28,960 --> 00:08:34,960
Alors, c'est texte ou image, mais avec un input visuel aussi.

83
00:08:34,960 --> 00:08:38,960
Donc, en fait, il y a plein d'outils qui sont sortis comme ça et qui permettent...

84
00:08:38,960 --> 00:08:42,960
Tu dessines, en fait, et en même temps que tu dessines, tu as l'intelligence artificielle

85
00:08:42,960 --> 00:08:49,960
avec l'éclairage d'un prompt qui te fait une adaptation de ton dessin, version plus jolie.

86
00:08:50,960 --> 00:08:54,960
Et donc, ça, c'était le truc qui faisait fureur ces deux dernières semaines.

87
00:08:54,960 --> 00:09:03,960
Le truc est sorti, tous les outils, toutes les applications qui étaient un petit peu dans ce genre de...

88
00:09:03,960 --> 00:09:08,960
Dans ce domaine-là, ils ont intégré cette tech dans leur outil.

89
00:09:08,960 --> 00:09:11,960
Et puis voilà, maintenant, c'est la compétition.

90
00:09:13,960 --> 00:09:16,960
Et donc, là, il y a un truc qui vient d'arriver.

91
00:09:16,960 --> 00:09:19,960
C'est le... Alors, attends que je retrouve, parce que je suis perdu dans mes onglets.

92
00:09:19,960 --> 00:09:22,960
C'est le texte ou...

93
00:09:22,960 --> 00:09:23,960
Alors, attends.

94
00:09:23,960 --> 00:09:24,960
Vidéo?

95
00:09:24,960 --> 00:09:25,960
Je suis perdu.

96
00:09:26,960 --> 00:09:28,960
Oui, c'est ça, c'est la vidéo, mais alors attends.

97
00:09:28,960 --> 00:09:30,960
Ah oui, voilà, c'est ça.

98
00:09:30,960 --> 00:09:36,960
Parce qu'en même temps, il y avait aussi un truc qui était pas mal à la mode ces deux dernières semaines.

99
00:09:36,960 --> 00:09:37,960
C'était aussi le...

100
00:09:38,960 --> 00:09:41,960
En fait, c'est pas nouveau au SEX, déjà depuis le upscaling,

101
00:09:41,960 --> 00:09:44,960
c'était quelque chose qui était quand même pas mal utilisé depuis un petit moment.

102
00:09:44,960 --> 00:09:49,960
Mais je sais pas pourquoi, tout d'un coup, toutes les applications se sont mis à en proposer,

103
00:09:49,960 --> 00:09:57,960
parce qu'il y a Magnify qui a sorti un truc qui est considéré comme étant supérieur aux autres.

104
00:09:57,960 --> 00:10:00,960
J'ai pas très bien compris exactement encore pourquoi, mais en tout cas, voilà.

105
00:10:00,960 --> 00:10:01,960
Donc, tout le monde s'y est mis.

106
00:10:02,960 --> 00:10:07,960
Et le upscaling, c'était essentiellement pour l'image, en fait.

107
00:10:09,960 --> 00:10:12,960
Donc, tu mets une image, tu lui demandes d'augmenter la résolution,

108
00:10:12,960 --> 00:10:16,960
et tu peux paramétrer, si tu veux, le degré d'interprétation de l'IA

109
00:10:16,960 --> 00:10:22,960
pour savoir à quel point il va se rapprocher de l'image originale,

110
00:10:22,960 --> 00:10:23,960
ou bien inventer quelque chose de nouveau.

111
00:10:24,960 --> 00:10:27,960
Et maintenant, ça y est, c'est sorti, il y a le upscaling vidéo.

112
00:10:27,960 --> 00:10:33,960
Donc, on peut mettre une vidéo en input et mettre le même genre de paramètres,

113
00:10:33,960 --> 00:10:35,960
et ça fait du upscaling.

114
00:10:35,960 --> 00:10:39,960
Alors, j'ai vu des exemples avec des vieux dessins animés qui sont qualité un peu,

115
00:10:39,960 --> 00:10:40,960
c'est un petit peu flou, etc.

116
00:10:40,960 --> 00:10:46,960
Les médecins dans l'IA, t'as la même chose, mais en super haute définition.

117
00:10:46,960 --> 00:10:52,960
Il y a eu, si je peux rebondir là-dessus, il y a eu des petits scandales

118
00:10:52,960 --> 00:10:56,960
sur des films qui ont été colorisés et qui n'ont pas été colorisés correctement.

119
00:10:57,960 --> 00:11:01,960
Donc, justement, ça va donner lieu, je pense, à des discussions intéressantes

120
00:11:01,960 --> 00:11:05,960
sur peut-être l'erreur et ce qu'on appelle l'erreur ou pas, en fait.

121
00:11:05,960 --> 00:11:09,960
Des fois, les erreurs de l'intelligence artificielle sont intéressantes

122
00:11:09,960 --> 00:11:13,960
parce que ça pose la question de ce qui était correct.

123
00:11:13,960 --> 00:11:17,960
Ou par exemple, quand on parlait du beau un tout petit peu plus tôt,

124
00:11:17,960 --> 00:11:19,960
de ce qu'est le beau, en fait.

125
00:11:20,960 --> 00:11:24,960
Souvent, le beau, c'est considéré comme, en tout cas, j'ai vu beaucoup

126
00:11:24,960 --> 00:11:29,960
de rapprochements l'année dernière suite à ce fameux concours

127
00:11:30,960 --> 00:11:35,960
qui soulevait le débat d'une œuvre d'art générée par une IA.

128
00:11:37,960 --> 00:11:41,960
Il y avait cette question de, est-ce que l'image était belle?

129
00:11:41,960 --> 00:11:47,960
Et en fait, le beau était aussi souvent lié dans les débats au figuratif, en fait.

130
00:11:49,960 --> 00:11:55,960
Au fait que l'image représentait la réalité de façon très fidèle ou non.

131
00:11:55,960 --> 00:11:58,960
Et donc, pour les historiens de l'art, c'est très perturbant

132
00:11:58,960 --> 00:12:03,960
parce que ce sont des questions qui ont été réglées par l'art contemporain

133
00:12:03,960 --> 00:12:08,960
au début du XXe siècle, avec l'explosion de l'abstraction

134
00:12:10,960 --> 00:12:13,960
et aussi avec les ready-made de Marcel Duchamp.

135
00:12:13,960 --> 00:12:17,960
Donc, en fait, ça fait un petit moment que les historiens de l'art

136
00:12:17,960 --> 00:12:20,960
ne se posent plus la question de l'esthétique,

137
00:12:20,960 --> 00:12:22,960
enfin la question du beau, de l'art.

138
00:12:22,960 --> 00:12:25,960
Parce qu'en histoire de l'art, l'esthétique, c'est autre chose.

139
00:12:25,960 --> 00:12:29,960
Tout comme c'est autre chose en philosophie et en anthropologie.

140
00:12:29,960 --> 00:12:34,960
Mais la vraie question du beau, c'est plus un créateur d'appréciation

141
00:12:34,960 --> 00:12:38,960
pour l'historien de l'art et même pour le marché de l'art.

142
00:12:38,960 --> 00:12:41,960
Pourtant, dans la tête du public, quand on génère une image

143
00:12:41,960 --> 00:12:43,960
par intelligence artificielle, elle revient.

144
00:12:43,960 --> 00:12:46,960
Donc, c'est intéressant parce qu'il y a un retour des questions

145
00:12:46,960 --> 00:12:48,960
de définition de l'art.

146
00:12:48,960 --> 00:12:51,960
Et c'est un bel exemple qui me permet de souligner ce que je disais

147
00:12:51,960 --> 00:12:56,960
plus tôt, qui était qu'en fait, l'IA, c'est l'occasion de voir

148
00:12:56,960 --> 00:12:59,960
comment fonctionne la société.

149
00:12:59,960 --> 00:13:02,960
Et donc là, par exemple, les critères d'appréciation de l'art

150
00:13:02,960 --> 00:13:04,960
ne sont pas les mêmes.

151
00:13:04,960 --> 00:13:08,960
En fait, quand on a un débat suite à un concours sur la place publique

152
00:13:08,960 --> 00:13:14,960
et dans les petits cercles de spécialistes,

153
00:13:15,960 --> 00:13:19,960
aussi bien à l'université que dans une galerie ou dans un musée,

154
00:13:19,960 --> 00:13:22,960
là encore, les critères ne sont pas tout à fait les mêmes.

155
00:13:22,960 --> 00:13:27,960
Donc, en fait, ça permet de faire de la sociologie de l'art aussi.

156
00:13:27,960 --> 00:13:32,960
Et donc, pour la question, par exemple, de définition de l'image,

157
00:13:32,960 --> 00:13:37,960
d'upscaler la définition de l'image, il y a eu cette...

158
00:13:37,960 --> 00:13:45,960
Ce qui est intéressant, c'est qu'il y a la question de la vérité de l'image.

159
00:13:45,960 --> 00:13:48,960
Si elle est de meilleure qualité, entre guillemets,

160
00:13:48,960 --> 00:13:51,960
on estime qu'on va mieux voir une image,

161
00:13:51,960 --> 00:13:54,960
alors que c'est l'intelligence artificielle qui a supposé que

162
00:13:54,960 --> 00:13:57,960
et qui a créé une image de meilleure qualité.

163
00:13:57,960 --> 00:14:02,960
C'est la même chose pour les images colorisées dont je parlais.

164
00:14:02,960 --> 00:14:05,960
Et donc, parfois, ça va forcément créer des petites erreurs.

165
00:14:05,960 --> 00:14:07,960
Il y aura des petites erreurs.

166
00:14:07,960 --> 00:14:12,960
Et en fait, il y a plusieurs personnes qui travaillent sur l'histoire contrefactuelle,

167
00:14:12,960 --> 00:14:17,960
dont Grégory Chatonski, sur de s'imaginer notamment

168
00:14:17,960 --> 00:14:22,960
qu'il aurait pu y avoir une autre histoire que l'histoire qui est présentée

169
00:14:22,960 --> 00:14:25,960
et que l'IA invente cette histoire alternative.

170
00:14:25,960 --> 00:14:30,960
Donc, par exemple, si on corrige une image en disant qu'on va proposer

171
00:14:30,960 --> 00:14:33,960
une image plus précise, mais que la correction n'est pas bonne,

172
00:14:33,960 --> 00:14:36,960
on propose une réalité alternative.

173
00:14:39,960 --> 00:14:41,960
C'est ça qui est fou, en fait.

174
00:14:41,960 --> 00:14:47,960
Parce que jusqu'à maintenant, avoir une image de meilleure résolution,

175
00:14:47,960 --> 00:14:50,960
c'était quelque part...

176
00:14:50,960 --> 00:14:54,960
C'était... Comment dire?

177
00:14:54,960 --> 00:14:56,960
C'était quelque part... C'était quelque chose de...

178
00:14:56,960 --> 00:14:57,960
J'ai oublié le mot.

179
00:14:57,960 --> 00:15:01,960
C'était quelque chose... On était plus proche de la vérité, en fait.

180
00:15:01,960 --> 00:15:03,960
On était plus proche de la réalité.

181
00:15:03,960 --> 00:15:05,960
Je ne suis pas tout à fait sûre de ça.

182
00:15:05,960 --> 00:15:08,960
Je pense qu'en général, oui.

183
00:15:08,960 --> 00:15:12,960
Mais je pense qu'il faut le mettre en perspective aussi, là encore,

184
00:15:12,960 --> 00:15:17,960
avec l'histoire où, par exemple, il y a eu des manipulations de photos.

185
00:15:17,960 --> 00:15:20,960
Il y a eu beaucoup de manipulations de photos

186
00:15:20,960 --> 00:15:23,960
avant l'apparition de l'intelligence artificielle.

187
00:15:23,960 --> 00:15:25,960
Alors, j'avais un exemple l'autre jour.

188
00:15:25,960 --> 00:15:27,960
J'avais un exemple il n'y a pas longtemps.

189
00:15:27,960 --> 00:15:30,960
J'étais dans un atelier d'ethnographie

190
00:15:30,960 --> 00:15:33,960
pour... C'est une formation qu'on donne aux doctorants

191
00:15:33,960 --> 00:15:39,960
pour vérifier qu'ils restent éthiques dans leurs ateliers d'ethnographie.

192
00:15:39,960 --> 00:15:42,960
Et donc, en fait, on parlait des entretiens,

193
00:15:42,960 --> 00:15:45,960
de faire des entretiens avec, dans mon cas, des artistes

194
00:15:45,960 --> 00:15:48,960
et dans les cas d'autres doctorants, d'autres personnes,

195
00:15:48,960 --> 00:15:50,960
et de restituer ces entretiens

196
00:15:50,960 --> 00:15:53,960
et de donner des preuves de recherche dans notre travail.

197
00:15:53,960 --> 00:15:57,960
Et Alain Chemut, qui était le sociologue qui animait ce débat,

198
00:15:57,960 --> 00:16:00,960
nous expliquait qu'il y avait un anthropologue,

199
00:16:00,960 --> 00:16:03,960
début XXe siècle, il me semble, premier quart,

200
00:16:03,960 --> 00:16:10,960
qui était allé voir les personnes qu'à l'époque on appelait les Eskimos

201
00:16:10,960 --> 00:16:12,960
et qu'aujourd'hui on appelle les Inuits,

202
00:16:12,960 --> 00:16:14,960
et qu'il les avait prises.

203
00:16:14,960 --> 00:16:16,960
Il a voulu les prendre en photo.

204
00:16:16,960 --> 00:16:19,960
Le problème, c'est qu'à l'époque, le matériel étant tellement compliqué

205
00:16:19,960 --> 00:16:24,960
à trimballer et les conditions du terrain ethnographique

206
00:16:24,960 --> 00:16:26,960
étant tellement compliquées,

207
00:16:26,960 --> 00:16:29,960
il avait finalement pris une photo

208
00:16:29,960 --> 00:16:33,960
en faisant croire que la photo avait été prise sur le terrain,

209
00:16:33,960 --> 00:16:34,960
une photo d'un Eskimo,

210
00:16:34,960 --> 00:16:37,960
et finalement c'était une photo, et on a découvert un peu plus tard,

211
00:16:37,960 --> 00:16:39,960
que c'était une photo qu'il avait prise lui-même chez lui

212
00:16:39,960 --> 00:16:41,960
une fois rentré chez lui,

213
00:16:41,960 --> 00:16:43,960
avec les vêtements qu'il avait récupérés

214
00:16:43,960 --> 00:16:46,960
et les armes qu'il avait récupérées sur le terrain.

215
00:16:46,960 --> 00:16:50,960
Donc, après, c'est posé la question de la valeur de la photo

216
00:16:50,960 --> 00:16:52,960
d'un point de vue anthropologique,

217
00:16:52,960 --> 00:16:55,960
puisque est-ce que la position était exactement la même,

218
00:16:55,960 --> 00:16:58,960
est-ce que les vêtements étaient portés exactement de la même façon?

219
00:16:58,960 --> 00:17:02,960
Donc, la question de l'apport documentaire de la photo

220
00:17:02,960 --> 00:17:04,960
et de la valeur scientifique de la photo,

221
00:17:04,960 --> 00:17:07,960
puis plus tard, de la valeur historique de la photo,

222
00:17:07,960 --> 00:17:09,960
se pose là alors,

223
00:17:09,960 --> 00:17:11,960
et ça montre très bien que même avant,

224
00:17:11,960 --> 00:17:13,960
une technique d'intelligence artificielle,

225
00:17:13,960 --> 00:17:14,960
on pouvait manipuler des photos.

226
00:17:14,960 --> 00:17:17,960
Il y a eu aussi beaucoup de manipulations de photos pendant les guerres,

227
00:17:17,960 --> 00:17:18,960
bien sûr,

228
00:17:19,960 --> 00:17:25,960
et même des tentatives de retouches de photos aussi,

229
00:17:25,960 --> 00:17:27,960
pour améliorer la qualité d'une photo,

230
00:17:27,960 --> 00:17:29,960
avant l'intelligence artificielle,

231
00:17:29,960 --> 00:17:31,960
qui pouvaient encore altérer la photo

232
00:17:31,960 --> 00:17:32,960
et faire croire que,

233
00:17:32,960 --> 00:17:36,960
notamment avec les changements de luminosité,

234
00:17:36,960 --> 00:17:38,960
on peut complètement changer les couleurs d'une photo.

235
00:17:38,960 --> 00:17:41,960
Donc, en fait, je pense que là encore,

236
00:17:41,960 --> 00:17:44,960
l'intelligence artificielle va exacerber un problème qu'on avait déjà,

237
00:17:44,960 --> 00:17:47,960
qui était de connaître la véracité d'un document

238
00:17:47,960 --> 00:17:51,960
et de, en fait, la valeur qu'on accorde à un document.

239
00:17:51,960 --> 00:17:54,960
J'ai jamais eu de traces,

240
00:17:54,960 --> 00:17:59,960
j'ai jamais eu la ressource bibliographique de cette information,

241
00:17:59,960 --> 00:18:01,960
mais je me rappelle qu'en formation,

242
00:18:01,960 --> 00:18:04,960
quand j'étais plus jeune en licence d'histoire de l'art,

243
00:18:04,960 --> 00:18:06,960
il y avait une professeure de communication de Sorbonne Université

244
00:18:06,960 --> 00:18:09,960
qui nous expliquait que quand la photo était apparue,

245
00:18:09,960 --> 00:18:12,960
la photo avait moins de valeur que le dessin,

246
00:18:12,960 --> 00:18:15,960
tout simplement parce que le rapport au dessin n'est pas le même,

247
00:18:15,960 --> 00:18:18,960
il n'était pas le même que le rapport à la photographie.

248
00:18:18,960 --> 00:18:20,960
Donc, en fait, sur les champs de bataille pendant la guerre,

249
00:18:22,960 --> 00:18:25,960
la photo étant quelque chose de relativement nouveau

250
00:18:25,960 --> 00:18:28,960
et de peu utilisé par le grand public,

251
00:18:28,960 --> 00:18:31,960
on préférait encore avoir recours au dessin

252
00:18:31,960 --> 00:18:33,960
parce que c'était la technique qu'on avait toujours utilisée.

253
00:18:33,960 --> 00:18:36,960
Et donc, en fait, ça, ça nous ramène à un philosophe

254
00:18:36,960 --> 00:18:38,960
qui s'appelle Simon Non,

255
00:18:38,960 --> 00:18:42,960
qui parle du mode d'existence des objets techniques,

256
00:18:42,960 --> 00:18:45,960
qui dit que les objets techniques ont plusieurs façons d'exister.

257
00:18:45,960 --> 00:18:49,960
Et en fait, c'est vrai que la confiance qu'on accorde aux objets techniques,

258
00:18:49,960 --> 00:18:51,960
on a tendance à croire qu'elle est évidente,

259
00:18:51,960 --> 00:18:54,960
alors qu'elle est culturellement imprimée.

260
00:18:54,960 --> 00:18:57,960
C'est parce qu'on choisit d'avoir plus confiance

261
00:18:57,960 --> 00:19:00,960
dans un algorithme que dans un humain

262
00:19:00,960 --> 00:19:05,960
qu'on pense que l'algorithme sera forcément plus juste que l'humain,

263
00:19:05,960 --> 00:19:08,960
ou l'inverse, en fait.

264
00:19:09,960 --> 00:19:13,960
C'est quoi les critères sur lesquels on se base pour donner plus de confiance?

265
00:19:13,960 --> 00:19:17,960
Par exemple, si on donne une photo, une vieille photo, justement,

266
00:19:17,960 --> 00:19:20,960
disons de la guerre, puisqu'on est sur ce sujet-là,

267
00:19:20,960 --> 00:19:23,960
à restaurer, qu'on le donne à un humain,

268
00:19:23,960 --> 00:19:27,960
un spécialiste de la restauration ou à une IA,

269
00:19:27,960 --> 00:19:32,960
on va donner quoi comme critères à l'IA pour qu'elle respecte?

270
00:19:32,960 --> 00:19:36,960
Parce qu'en fait, il y a une dimension herméneutique.

271
00:19:36,960 --> 00:19:39,960
C'est comment on respecte le truc d'origine,

272
00:19:39,960 --> 00:19:42,960
à quel point on va dans la création, etc.

273
00:19:42,960 --> 00:19:45,960
Et comment on fait pour savoir c'est quoi la bonne pratique,

274
00:19:45,960 --> 00:19:47,960
qu'est-ce qu'il faut faire?

275
00:19:47,960 --> 00:19:52,960
Finalement, il faut essayer d'insuffler à l'IA

276
00:19:52,960 --> 00:19:56,960
une espèce de réflexion philosophique par rapport à l'objet qu'on lui donne.

277
00:19:56,960 --> 00:19:57,960
Je ne pense pas.

278
00:19:57,960 --> 00:19:59,960
C'est juste que je pense qu'en fait, il faut que nous,

279
00:19:59,960 --> 00:20:01,960
quand on l'utilise, si on décide de faire ça,

280
00:20:01,960 --> 00:20:03,960
de faire ça de comparaison,

281
00:20:03,960 --> 00:20:08,960
on est conscients des biais aussi bien de l'humain qui va restaurer

282
00:20:08,960 --> 00:20:13,960
que de l'IA qui va travailler sur la photo.

283
00:20:13,960 --> 00:20:15,960
Et donc, dans les deux cas, les biais seront différents.

284
00:20:15,960 --> 00:20:18,960
Bien sûr, dans l'IA, ça va être le paramétrage,

285
00:20:18,960 --> 00:20:20,960
ça va être le dataset,

286
00:20:20,960 --> 00:20:23,960
ça va être les données sur lesquelles elle va s'appuyer.

287
00:20:23,960 --> 00:20:27,960
Et dans le cas de l'humain, ça va être les données

288
00:20:27,960 --> 00:20:31,960
que cette personne a en tête, les techniques qu'elle a en tête,

289
00:20:31,960 --> 00:20:32,960
ce qu'elle a lu.

290
00:20:32,960 --> 00:20:35,960
Ce n'est pas si différent en réalité.

291
00:20:35,960 --> 00:20:39,960
Mais je pense que dans les deux cas, il faut avoir un regard critique,

292
00:20:39,960 --> 00:20:44,960
tout comme exactement quand un chercheur produit quelque chose,

293
00:20:44,960 --> 00:20:47,960
il a parfaitement conscience que lui-même est biaisé

294
00:20:47,960 --> 00:20:51,960
et qu'il ne produira jamais quelque chose de complètement neutre.

295
00:20:51,960 --> 00:20:55,960
Et le lecteur va le lire en ayant conscience qu'il a une vision des choses,

296
00:20:55,960 --> 00:20:58,960
même si elle est soutenue par une série d'arguments

297
00:20:58,960 --> 00:21:00,960
et par une méthode très cadrée.

298
00:21:00,960 --> 00:21:01,960
C'est toujours pareil.

299
00:21:01,960 --> 00:21:03,960
Et le truc, c'est que l'intelligence artificielle,

300
00:21:03,960 --> 00:21:06,960
elle est aussi biaisée que, pour moi,

301
00:21:06,960 --> 00:21:09,960
elle est aussi biaisée qu'un humain.

302
00:21:10,960 --> 00:21:11,960
Voire une.

303
00:21:12,960 --> 00:21:14,960
Et tu vois, je pense qu'on est doomed quelque part,

304
00:21:14,960 --> 00:21:16,960
parce que j'étais en train de me dire,

305
00:21:16,960 --> 00:21:19,960
en fait, pour la photo, c'est assez simple au final.

306
00:21:19,960 --> 00:21:24,960
Il suffit d'alimenter l'IA avec suffisamment de photos du même type,

307
00:21:24,960 --> 00:21:29,960
éventuellement même avec du metadata

308
00:21:29,960 --> 00:21:32,960
qui permet d'augmenter encore sa pertinence, etc.

309
00:21:33,960 --> 00:21:36,960
Je me suis dit, ok, ça c'est bon.

310
00:21:36,960 --> 00:21:37,960
L'IA, elle est meilleure.

311
00:21:37,960 --> 00:21:41,960
Après, j'ai essayé de réfléchir à un cas

312
00:21:41,960 --> 00:21:44,960
où vraiment l'humain ne pourrait pas être remplacé.

313
00:21:44,960 --> 00:21:47,960
Et je pensais à ce truc-là.

314
00:21:48,960 --> 00:21:51,960
Alors, en musique, il y a des problèmes.

315
00:21:51,960 --> 00:21:56,960
Des fois, on a des œuvres où, en fait, on n'a pas toute la partition.

316
00:21:56,960 --> 00:21:59,960
Et donc, les chefs d'orchestre, qu'est-ce qu'ils font?

317
00:21:59,960 --> 00:22:01,960
Ils sont obligés de combler les trous.

318
00:22:01,960 --> 00:22:05,960
Donc, du coup, c'est intéressant parce que ça fait des œuvres

319
00:22:05,960 --> 00:22:09,960
qui sont uniques quelque part en fonction de quel est le chef d'orchestre

320
00:22:09,960 --> 00:22:11,960
qui a pris en main ce truc-là.

321
00:22:11,960 --> 00:22:15,960
Il y a une œuvre à laquelle je pense, plus précisément,

322
00:22:15,960 --> 00:22:21,960
c'est une œuvre, comment dire, par rock, si je ne dis pas de conneries,

323
00:22:21,960 --> 00:22:24,960
de membrage des Industries, de Louxteau,

324
00:22:24,960 --> 00:22:28,960
qui était un compositeur juste avant Bach.

325
00:22:28,960 --> 00:22:32,960
Et en gros, il y a une partie, mais c'est quand même assez long.

326
00:22:32,960 --> 00:22:34,960
C'est une page au complet qui manque.

327
00:22:34,960 --> 00:22:39,960
Et quand ce passage-là arrive, c'est totalement une création

328
00:22:39,960 --> 00:22:45,960
d'interprétation.

329
00:22:45,960 --> 00:22:49,960
Et en fait, ce qu'ils essaient de faire, les types,

330
00:22:49,960 --> 00:22:53,960
c'est qu'évidemment, ils essaient de coller à l'esprit au maximum

331
00:22:53,960 --> 00:22:56,960
de ce qui entoure ce moment-là.

332
00:22:56,960 --> 00:23:01,960
Ils ne vont pas se mettre à faire du jazz bebop.

333
00:23:01,960 --> 00:23:05,960
Mais il y a quand même, j'imagine qu'il doit y avoir des œuvres,

334
00:23:05,960 --> 00:23:10,960
des projets où les mecs se disent, je vais être un peu plus flyé.

335
00:23:10,960 --> 00:23:13,960
En fait, c'est quoi, comme tu disais un petit peu au début,

336
00:23:13,960 --> 00:23:19,960
c'est quoi le juste remplissage?

337
00:23:19,960 --> 00:23:23,960
Comment on définit ça? Qu'est-ce qui est bien?

338
00:23:23,960 --> 00:23:27,960
Et en fait, pour l'instant, je pense que ce n'est pas encore ça.

339
00:23:27,960 --> 00:23:32,960
Mais au final, avec assez de paramètres, assez de data,

340
00:23:32,960 --> 00:23:35,960
c'est un parfait moment.

341
00:23:35,960 --> 00:23:39,960
Après, les données, elles seront toujours biaisées.

342
00:23:40,960 --> 00:23:45,960
C'est impossible de ne pas avoir de biais, je pense.

343
00:23:45,960 --> 00:23:49,960
C'est une illusion de penser qu'on peut être neutre.

344
00:23:49,960 --> 00:23:52,960
Si les sciences humaines et sociales ont essayé d'être neutres

345
00:23:52,960 --> 00:23:56,960
pendant des années, des années, finalement ont décidé d'abandonner le projet

346
00:23:56,960 --> 00:23:59,960
dans les années 70, c'est que ce n'est pas possible.

347
00:23:59,960 --> 00:24:03,960
Mais après, avec les données, c'est un peu différent encore.

348
00:24:03,960 --> 00:24:06,960
Je pense qu'on peut pousser vers une neutralité déjà.

349
00:24:06,960 --> 00:24:09,960
C'est important qu'on le fasse.

350
00:24:09,960 --> 00:24:12,960
Et après, pour la musique, pour cette question-là,

351
00:24:12,960 --> 00:24:17,960
que j'avais effectivement vue passer et qui est absolument passionnante,

352
00:24:17,960 --> 00:24:21,960
je ne suis pas sûre qu'il y ait de juste réponse.

353
00:24:21,960 --> 00:24:25,960
On ne saura jamais ce qu'aurait été cette partition

354
00:24:25,960 --> 00:24:28,960
si elle avait été terminée.

355
00:24:28,960 --> 00:24:31,960
Et donc, quelque part, est-ce que ce n'est pas la richesse

356
00:24:31,960 --> 00:24:38,960
de ce trou-là que de voir le remplissage

357
00:24:38,960 --> 00:24:41,960
et de voir comment chacun interprète ce remplissage?

358
00:24:44,960 --> 00:24:47,960
Complètement. Après, ce que je me disais rapidement en tête,

359
00:24:47,960 --> 00:24:50,960
c'était que je me disais, en même temps, ce travail-là,

360
00:24:50,960 --> 00:24:53,960
ça demande, si tu veux faire quelque chose de fidèle, de cohérent,

361
00:24:53,960 --> 00:24:57,960
ça demande un certain travail de recherche de l'auteur, du compositeur.

362
00:24:57,960 --> 00:25:00,960
Qu'est-ce qu'il faisait à cette époque-là?

363
00:25:00,960 --> 00:25:03,960
Quel était son style en général?

364
00:25:03,960 --> 00:25:07,960
Mais aussi, est-ce qu'il était dans une période où il faisait plus ce genre de choses?

365
00:25:07,960 --> 00:25:11,960
Bref, toutes sortes de choses que l'IA ne fait pas forcément,

366
00:25:11,960 --> 00:25:14,960
mais peut-être qu'à un moment donné, l'IA le fera.

367
00:25:14,960 --> 00:25:16,960
C'est ça que je me dis en fait.

368
00:25:16,960 --> 00:25:17,960
C'est possible.

369
00:25:20,960 --> 00:25:23,960
En tout cas, tout ça, c'est...

370
00:25:23,960 --> 00:25:28,960
Du coup, c'est ça, on s'est un peu étalés sur le upscaling.

371
00:25:28,960 --> 00:25:30,960
Mais en gros, c'est vrai que c'était pas mal.

372
00:25:30,960 --> 00:25:33,960
Maintenant, il y a de toutes sortes de trucs.

373
00:25:33,960 --> 00:25:39,960
Il y a un outil qui vient de sortir qui fait du text to AR.

374
00:25:39,960 --> 00:25:42,960
Text to réalité augmentée.

375
00:25:42,960 --> 00:25:47,960
Donc, tu peux mettre un filtre sur ton visage en fonction de ce que tu tapes dans le prompt.

376
00:25:49,960 --> 00:25:52,960
Il s'appelle Shader, pour ceux qui sont intéressés.

377
00:25:53,960 --> 00:25:55,960
Apparemment, c'est possible.

378
00:25:55,960 --> 00:25:58,960
Alors ça, c'est vraiment anecdotique, mais je vais quand même en parler.

379
00:25:58,960 --> 00:26:05,960
Apparemment, c'est possible maintenant d'utiliser des images générées par l'intelligence artificielle

380
00:26:05,960 --> 00:26:12,960
dans son téléphone de la même manière que les smileys et les GIFs de GIFi.

381
00:26:12,960 --> 00:26:16,960
Maintenant, si vous téléchargez sur iOS ou Android,

382
00:26:16,960 --> 00:26:20,960
l'application qui s'appelle Microsoft GIFi,

383
00:26:20,960 --> 00:26:23,960
Microsoft SwiftKey Keyboard,

384
00:26:23,960 --> 00:26:31,960
ça rajoute un onglet à côté des trucs là, stickers, GIFs, smileys.

385
00:26:31,960 --> 00:26:36,960
Et vous pouvez traper un prompt et insérer n'importe quelle image dans le prompt.

386
00:26:36,960 --> 00:26:39,960
C'est vrai, je n'avais absolument aucune idée de ça.

387
00:26:41,960 --> 00:26:42,960
J'ai essayé.

388
00:26:44,960 --> 00:26:49,960
Je suis à la pointe des nouveautés qui ne servent à rien, mais c'est quand même intéressant.

389
00:26:51,960 --> 00:26:53,960
Je suis sûre qu'il y a des utilisations très utiles.

390
00:26:55,960 --> 00:26:57,960
Ça viendra.

391
00:26:59,960 --> 00:27:00,960
C'est sûr.

392
00:27:00,960 --> 00:27:05,960
Je n'ai malheureusement pas assez de temps pour essayer de trouver les trucs.

393
00:27:05,960 --> 00:27:11,960
C'est fou parce que souvent, des outils d'IA, j'en vois passer tout le temps, tout le temps.

394
00:27:11,960 --> 00:27:17,960
Et à chaque fois, je suis super impressionné par toutes les possibilités que ça ouvre.

395
00:27:17,960 --> 00:27:20,960
Mais en fait, je n'ai pas le temps, je fais juste des trucs de merde.

396
00:27:20,960 --> 00:27:22,960
Et après, je fais, oui, c'est bon, je l'ai essayé.

397
00:27:22,960 --> 00:27:28,960
Et après, je vois des gens hyper créatifs qui font des trucs, qui imaginent des emplois vraiment très spécifiques.

398
00:27:28,960 --> 00:27:30,960
Je suis, oh putain, ça c'est bon.

399
00:27:30,960 --> 00:27:31,960
J'aurais tellement aimé cette idée.

400
00:27:32,960 --> 00:27:33,960
Oui.

401
00:27:33,960 --> 00:27:34,960
Aimer, avoir.

402
00:27:34,960 --> 00:27:35,960
C'est le problème.

403
00:27:35,960 --> 00:27:41,960
En plus, ça va tellement vite que c'est très difficile de travailler dessus parce que justement, on n'est jamais à la page.

404
00:27:41,960 --> 00:27:47,960
Et quand on veut prendre un petit peu le temps de se concentrer sur quelque chose, le temps de faire quelques recherches.

405
00:27:47,960 --> 00:27:54,960
Surtout quand on est historien, on essaie de remonter quand même un peu en arrière, donner de la profondeur historique au truc et voir s'il y a vraiment une nouveauté ou pas.

406
00:27:56,960 --> 00:28:01,960
À peine, on a eu le temps de travailler sur ça, que le lendemain, il y a déjà quelque chose d'autre qui est sorti.

407
00:28:01,960 --> 00:28:06,960
Donc, c'est vrai que c'est un vrai challenge de travailler là-dessus pour ça.

408
00:28:07,960 --> 00:28:08,960
Ah oui, mais c'est vrai.

409
00:28:08,960 --> 00:28:11,960
D'ailleurs, c'est vraiment totalement une question que j'ai envie de te poser.

410
00:28:11,960 --> 00:28:28,960
Comment tu fais pour faire un travail qui, genre un travail de, que ce soit dans ton travail de recherche avec, comment il s'appelle, Culturia ou alors dans le cadre de ton doctorat.

411
00:28:28,960 --> 00:28:30,960
Comment tu fais pour Keep Up?

412
00:28:31,960 --> 00:28:38,960
En fait, c'est des trucs long terme et tout d'un coup, tu travailles sur un sujet qui change, qui évolue de jour en jour.

413
00:28:38,960 --> 00:28:39,960
Comment ça se passe?

414
00:28:39,960 --> 00:28:40,960
C'est une excellente question.

415
00:28:40,960 --> 00:28:46,960
C'est une question à laquelle ma réponse ne me satisfait pas du tout.

416
00:28:46,960 --> 00:28:56,960
Alors pour Culturia, moi, je fais simplement une petite veille qui est surtout culturelle sur toutes les dimensions.

417
00:28:56,960 --> 00:28:59,960
Par exemple, la grève des acteurs m'a beaucoup intéressée.

418
00:28:59,960 --> 00:29:08,960
Donc là, c'est simplement se maintenir informé avec toute une série de newsletters d'un peu partout dans la mesure du possible.

419
00:29:08,960 --> 00:29:20,960
Mais voilà, c'est assez léger, sachant que tous les chercheurs qui travaillent sur ces questions se renseignent aussi de leur côté et que nos projets avancent de notre côté.

420
00:29:21,960 --> 00:29:30,960
Maintenant, pour ma thèse de doctorat, c'est une excellente question parce que j'ai une approche qui est un peu controversée chez les historiens.

421
00:29:30,960 --> 00:29:33,960
C'est-à-dire que je fais de l'histoire du temps présent.

422
00:29:33,960 --> 00:29:38,960
L'histoire du temps présent, c'est l'histoire en général des 30 dernières années.

423
00:29:38,960 --> 00:29:55,960
Et en fait, c'est un courant historique qui a été développé surtout à Sciences Po dans les années 90 et qui propose de revenir sur les 30 dernières années en partant du principe que les sources disponibles sont nombreuses.

424
00:29:55,960 --> 00:30:02,960
Puisqu'on a encore les personnes sur lesquelles on travaille qui sont vivantes, en général accessibles en fonction du sujet.

425
00:30:03,960 --> 00:30:07,960
Et donc, on peut faire des entretiens, on peut faire de ce qu'on appelle de l'histoire orale.

426
00:30:07,960 --> 00:30:16,960
Donc, discuter avec ces personnes pour mieux connaître leurs réponses à nos problématiques et à nos questions.

427
00:30:16,960 --> 00:30:26,960
Et en général, l'histoire du temps présent, elle a aussi cette particularité de s'intéresser aux dernières années et d'aller chercher dans les années précédentes.

428
00:30:26,960 --> 00:30:41,960
Donc, dans mon cas, dans toute l'histoire de l'art qui précède au XXIe siècle que j'étudie, des réponses pour savoir si les questions qu'on aborde et qui nous paraissent nouvelles sont vraiment nouvelles.

429
00:30:41,960 --> 00:30:49,960
Ou sont des questions qui ressortent de façon cyclique, des débats qui ressortent de façon cyclique dans nos cultures.

430
00:30:49,960 --> 00:30:57,960
Donc, il y a déjà ce parti-là. J'ai décidé de partir de 2001 et de m'arrêter aujourd'hui.

431
00:30:57,960 --> 00:31:01,960
C'est vrai qu'au bout d'un moment, j'ai posé la question avec ma directrice de thèse.

432
00:31:01,960 --> 00:31:08,960
Je lui ai demandé, je lui ai dit, mais est-ce que vous pensez que je devrais arrêter en 2022 parce que ça devient épuisant?

433
00:31:08,960 --> 00:31:20,960
Et en fait, on s'est dit que non parce qu'il y a le filtre de la question, la problématique de ma thèse qui est très précise.

434
00:31:20,960 --> 00:31:26,960
Ce n'est pas seulement l'intelligence artificielle dans l'art contemporain. Moi, je m'intéresse à la question de l'émotion.

435
00:31:26,960 --> 00:31:35,960
Donc, toutes les œuvres d'art et toutes les innovations techniques qui sortent sur l'intelligence artificielle ne sont pas liées intrinsèquement à l'émotion.

436
00:31:35,960 --> 00:31:40,960
Donc, tout ne va pas m'intéresser. En réalité, il y a certaines choses qui vont m'intéresser et d'autres pas.

437
00:31:40,960 --> 00:31:50,960
Donc, je vais filtrer ces choses qui m'intéressent et puis je vais garder ce qui me permet de faire ma démonstration.

438
00:31:50,960 --> 00:31:58,960
Je ne vais pas avoir besoin de couvrir exhaustivement toutes les œuvres qui ont été produites par l'intelligence artificielle

439
00:31:58,960 --> 00:32:08,960
et qui parlent d'émotion pour répondre à ma question. Je vais avoir quelques exemples assez forts qui vont suffire à soutenir ma thèse et ça suffira.

440
00:32:08,960 --> 00:32:18,960
Mais c'est vrai que ça demande d'avoir l'œil assez ouvert. Et pour être un peu moins flou, je vais peut-être vous parler rapidement de la question que je pose dans ma thèse.

441
00:32:19,960 --> 00:32:35,960
Pour faire simple et rapide, je travaille sur l'histoire de l'art et l'histoire des émotions.

442
00:32:35,960 --> 00:32:43,960
L'histoire des émotions, c'est un courant historiographique, là aussi récent, dans les années 90, qui étudie l'expression émotionnelle.

443
00:32:43,960 --> 00:32:53,960
Les historiens des émotions pensent que l'expression émotionnelle est différente en fonction de la période historique comme elle est différente en fonction de la zone géographique.

444
00:32:53,960 --> 00:32:59,960
Je parle de l'expression linguistique ou alors l'expression par les gestes ou alors aussi l'expression par des œuvres, par exemple.

445
00:32:59,960 --> 00:33:10,960
Les historiens des émotions se sont surtout intéressés aux mots qu'on utilisait pour exprimer les émotions et ils ont fait des études,

446
00:33:10,960 --> 00:33:18,960
notamment, surtout au Moyen-Âge, pour voir et pour montrer que les mots qu'on utilise dans une certaine période ne sont pas les mêmes que dans une autre période,

447
00:33:18,960 --> 00:33:22,960
non seulement parce qu'on n'a pas les mêmes façons de parler, mais aussi parce qu'on n'a pas du tout les mêmes normes.

448
00:33:22,960 --> 00:33:31,960
Il y a des émotions qu'on va exprimer dans un contexte et pas dans un autre et ces normes-là ne seront pas les mêmes au Moyen-Âge qu'au XXIe siècle.

449
00:33:32,960 --> 00:33:41,960
Et donc, eux se sont penchés sur les mots que les gens utilisent pour exprimer leurs émotions dans un contexte ou un autre et moi je me suis penchée sur les images.

450
00:33:41,960 --> 00:33:46,960
Je me suis dit que les images, c'était une source pour faire de l'histoire des émotions.

451
00:33:46,960 --> 00:33:54,960
Et donc, en fait, après ça, il fallait que je trouve un contexte quand j'ai découvert que je voulais bosser sur ça, sur les émotions

452
00:33:55,960 --> 00:33:59,960
et sur utiliser les images pour comprendre comment on exprimait nos émotions.

453
00:33:59,960 --> 00:34:04,960
Et donc, ce contexte, j'ai pensé à l'intelligence artificielle.

454
00:34:04,960 --> 00:34:09,960
Je me suis dit, mais est-ce que l'intelligence artificielle, ça change ou pas la façon dont on a d'exprimer nos émotions?

455
00:34:09,960 --> 00:34:21,960
Parce que l'histoire, c'est ça, c'est de voir est-ce qu'il y a des changements ou non dans le temps, en fait, pour comprendre les normes et comment elles s'imposent et comment on les transgresse.

456
00:34:22,960 --> 00:34:26,960
Et donc, ma question, c'est ça.

457
00:34:26,960 --> 00:34:35,960
Est-ce que le développement de l'IA depuis le début du 21e siècle, avec l'explosion du deep learning et du machine learning,

458
00:34:35,960 --> 00:34:46,960
et avec d'autres techniques, bien sûr, est-ce que l'IA depuis 2000-2001 change les façons dont on a d'exprimer nos émotions?

459
00:34:46,960 --> 00:35:00,960
Et donc, je vais voir des artistes et je vais regarder leur travail et voir si c'est vrai ou non, si ça change ou pas la façon dont on a de voir les choses.

460
00:35:00,960 --> 00:35:06,960
Et donc, j'ai trois axes pour l'instant qui me permettent d'explorer la question.

461
00:35:06,960 --> 00:35:13,960
Le premier, c'est les représentations de l'intelligence artificielle plus que les usages de l'IA.

462
00:35:13,960 --> 00:35:21,960
Et donc, est-ce qu'on va la représenter comme un fantasme, comme quelque chose d'extraordinaire qui nous arrive,

463
00:35:21,960 --> 00:35:28,960
ou est-ce qu'on va la représenter comme quelque chose de dramatique qui viendrait mettre un terme à l'humanité?

464
00:35:28,960 --> 00:35:35,960
Et donc, quand j'étudie ces modes de représentation, je vais voir quels codes on va utiliser.

465
00:35:35,960 --> 00:35:45,960
Donc, par exemple, la série Black Mirror m'intéresse énormément parce que c'est l'humour noir british par excellence

466
00:35:45,960 --> 00:35:52,960
qui va en fait utiliser l'IA comme un contexte, mais très souvent va jeter la lumière sur des vices plutôt humains,

467
00:35:52,960 --> 00:35:58,960
plutôt qu'au lieu de simplement blâmer la technique.

468
00:35:58,960 --> 00:36:04,960
Souvent on part avec une technique, la technique en soi n'est pas mauvaise, et puis ça dégénère

469
00:36:04,960 --> 00:36:13,960
parce que c'est l'humain derrière qui va utiliser cette technique et qui va projeter ses vices dans la technique.

470
00:36:13,960 --> 00:36:21,960
Et donc, là justement, il y a plusieurs aspects chez Black Mirror qui sont intéressants.

471
00:36:21,960 --> 00:36:33,960
C'est le fait qu'il y ait un humour particulièrement noir, et le fait aussi qu'il y ait Charlie Broker qui est le scénariste.

472
00:36:33,960 --> 00:36:36,960
C'est une série qui est portée par un scénariste et pas par un réalisateur.

473
00:36:36,960 --> 00:36:44,960
Charlie Broker explique que très souvent les gens venaient les voir et leur disaient « votre épisode, c'est réalisé ».

474
00:36:44,960 --> 00:36:58,960
Donc ça aussi c'est intéressant parce qu'on dit parfois que l'IA c'est un des rares domaines où l'art prédit l'avenir.

475
00:36:58,960 --> 00:37:08,960
Donc ça, ça donne une position à l'artiste qui est intéressante puisqu'il est parfois comparé au futurologue.

476
00:37:09,960 --> 00:37:13,960
Donc voilà, ça c'est un premier aspect.

477
00:37:13,960 --> 00:37:18,960
Le deuxième aspect c'est qu'il y a des artistes qui utilisent des IAs dans leurs œuvres, mais dans leur processus créatif,

478
00:37:18,960 --> 00:37:24,960
cette fois-ci pas dans le système de représentation, pour parler des nouvelles façons qu'on a d'interagir entre humains.

479
00:37:24,960 --> 00:37:29,960
Donc émotionnellement par exemple, on peut avoir de nouvelles interactions,

480
00:37:29,960 --> 00:37:36,960
notamment grâce à nos réseaux sociaux et les algorithmes qui vont nous suggérer tel ou tel contenu.

481
00:37:36,960 --> 00:37:45,960
Par exemple les chambres d'écho qui vont parfois encourager la colère pour encourager les interactions sur les réseaux

482
00:37:45,960 --> 00:37:48,960
et pour faire en sorte qu'on reste plus longtemps sur les réseaux.

483
00:37:48,960 --> 00:37:54,960
Donc ça va créer plus de couleurs, ça va créer plus de polarisation politique, etc.

484
00:37:54,960 --> 00:37:59,960
Des fois il y a des artistes qui vont s'emparer de ces sujets-là et qui vont créer des œuvres là-dessus.

485
00:38:00,960 --> 00:38:06,960
Ils vont utiliser l'IA pour étudier comment nous on évolue émotionnellement les uns avec les autres,

486
00:38:06,960 --> 00:38:12,960
mais aussi pas seulement d'humain à humain, mais aussi entre l'humain et le non-humain.

487
00:38:12,960 --> 00:38:18,960
Il y a beaucoup d'artistes qui utilisent l'IA pour réfléchir au rapport avec l'environnement,

488
00:38:18,960 --> 00:38:22,960
au rapport qu'on entretient avec l'environnement.

489
00:38:23,960 --> 00:38:30,960
Là encore c'est un rapport émotionnel parce qu'il y a tout un courant philosophique qui se développe en ce moment

490
00:38:30,960 --> 00:38:36,960
et depuis un bout de temps, avec par exemple des philosophes comme Baptiste Morisot qui disent qu'il faut développer

491
00:38:36,960 --> 00:38:41,960
une nouvelle sensibilité à la nature, une nouvelle diplomatie à l'égard de la nature, comme il dit.

492
00:38:41,960 --> 00:38:49,960
Donc ça aussi c'est un changement émotionnel parce que c'est carrément un changement ontologique pour reprendre certains philosophes.

493
00:38:49,960 --> 00:38:56,960
Il s'agit de changer le regard qu'on pose sur le non-humain, donc ça aussi l'IA peut nous aider à le faire.

494
00:38:56,960 --> 00:39:05,960
Et si on pousse ce sujet là, on arrive à la troisième partie, le troisième type d'œuvre qui m'intéresse.

495
00:39:05,960 --> 00:39:08,960
Ce sont les œuvres où on projette de l'humain sur les œuvres.

496
00:39:08,960 --> 00:39:15,960
Par exemple, il y a des personnes dont le métier c'est d'être concepteur d'émotions,

497
00:39:15,960 --> 00:39:25,960
donc c'est de créer des robots et de développer ces robots et ces intelligences artificielles pour qu'elles aillent le plus humaine possible.

498
00:39:25,960 --> 00:39:32,960
Par exemple, il y a des personnes dont le job c'était de donner de l'humour à Alexa,

499
00:39:32,960 --> 00:39:41,960
ou alors il y a des personnes dont le travail c'est de rendre un robot physiquement le plus humain possible

500
00:39:41,960 --> 00:39:49,960
pour qu'ensuite quand il interagisse avec son public, le public lui prête un maximum d'humanité.

501
00:39:49,960 --> 00:39:57,960
Pour moi, ces personnes-là sont très intéressantes, et ces créateurs-là et créatrices-là sont très intéressantes

502
00:39:57,960 --> 00:40:07,960
parce qu'elles vont projeter leur définition de l'humain et du sensible sur ces machines.

503
00:40:08,960 --> 00:40:18,960
Ce qui définit l'humain, l'émotion, le sensible, la créativité pour moi, n'est pas la même chose que pour une personne d'une autre culture.

504
00:40:22,960 --> 00:40:29,960
Justement, c'est là qu'il y a une excellente source pour comprendre les différences culturelles.

505
00:40:29,960 --> 00:40:37,960
Par exemple, je parlais avec un artiste français qui a développé un robot qui dessine.

506
00:40:37,960 --> 00:40:42,960
C'est un artiste qui s'appelle Patrick Tresset, qui est très intéressant.

507
00:40:42,960 --> 00:40:47,960
Pour lui, son robot qui dessine n'est pas du tout créatif.

508
00:40:47,960 --> 00:40:53,960
Il m'a dit que pour lui c'était juste un outil comme un autre,

509
00:40:53,960 --> 00:40:57,960
et que même si c'était très impressionnant et que oui, ça faisait partie de son œuvre,

510
00:40:57,960 --> 00:41:01,960
cet aspect impressionnant et cet aspect très théâtral,

511
00:41:01,960 --> 00:41:05,960
c'est même justement qu'il est presque une pièce de théâtre.

512
00:41:05,960 --> 00:41:14,960
Le public est tellement étonné par la capacité du robot à dessiner que l'artiste en joue,

513
00:41:14,960 --> 00:41:17,960
alors que pour lui, ce n'est qu'un outil.

514
00:41:17,960 --> 00:41:21,960
Il va même rendre son robot le plus humain possible pour tromper son public.

515
00:41:21,960 --> 00:41:27,960
C'est une façon de jouer avec son public, puisque lui-même dit que ce n'est qu'une machine.

516
00:41:27,960 --> 00:41:30,960
Et quand on compare cette vision de la créativité avec une autre,

517
00:41:30,960 --> 00:41:36,960
on s'aperçoit que finalement la définition de la créativité est très instable.

518
00:41:36,960 --> 00:41:41,960
On pourrait la comparer par exemple avec un artiste qui s'appelle Lionel Moura.

519
00:41:41,960 --> 00:41:48,960
Ces deux artistes-là ont été exposés dans l'exposition Artistes et Robots,

520
00:41:48,960 --> 00:41:54,960
qui avait été organisée par Laurence Bertrand-Dorléac, qui est ma directrice de recherche en 2018.

521
00:41:54,960 --> 00:41:57,960
Il me semble que c'était au Grand Palais.

522
00:41:57,960 --> 00:42:03,960
Et donc Lionel Moura, lui, il fait des petits robots qui portent un feutre

523
00:42:03,960 --> 00:42:07,960
et qui se déplacent avec des roulettes sur une toile.

524
00:42:07,960 --> 00:42:13,960
Donc ça n'a pas une forme anthropomorphe comme les robots de Patrick Tressay,

525
00:42:13,960 --> 00:42:18,960
qui ont un bras mécanisé, qui dessinent ce que perçoit leur caméra.

526
00:42:18,960 --> 00:42:22,960
Et donc ces petits robots qui se déplacent sur une toile,

527
00:42:22,960 --> 00:42:27,960
en fait ils se déplacent l'un par rapport à l'autre.

528
00:42:27,960 --> 00:42:33,960
C'est-à-dire que l'artiste a codé ces robots pour qu'ils aient à peu près la même intelligence que les fourmis ou que les abeilles.

529
00:42:33,960 --> 00:42:42,960
C'est une intelligence collective qui fait que les actions de l'un vont être conditionnées par les actions de l'autre.

530
00:42:42,960 --> 00:42:51,960
Donc le robot A se déplace en fonction du mouvement que vient de faire le robot B, et vice-versa.

531
00:42:51,960 --> 00:42:56,960
Ah ouais, ça s'appelle du swarm quelque chose, je ne sais plus le nom exact.

532
00:42:56,960 --> 00:43:01,960
Ça a un nom en français, mais je n'ai jamais le mot sur...

533
00:43:01,960 --> 00:43:04,960
C'est toujours le mot sur le bout de la langue.

534
00:43:04,960 --> 00:43:11,960
Je le tape là pour voir si je peux le retrouver.

535
00:43:12,960 --> 00:43:16,960
Mais non, je n'ai pas le mot.

536
00:43:16,960 --> 00:43:18,960
Je jure.

537
00:43:18,960 --> 00:43:22,960
C'est vrai qu'il est chiant de parler avec des humains comme toi.

538
00:43:22,960 --> 00:43:25,960
Si tu avais été un robot, ça aurait été beaucoup plus rapide.

539
00:43:25,960 --> 00:43:26,960
Exactement.

540
00:43:26,960 --> 00:43:31,960
Mais bon, en tout cas, il a codé ces robots sur l'intelligence des insectes.

541
00:43:31,960 --> 00:43:44,960
Et il a écrit un manifeste qui dit que selon lui, la créativité, c'est quelque chose qui se partage en dehors de l'espèce humaine,

542
00:43:44,960 --> 00:43:52,960
et qui peut se retrouver chez les insectes par ce processus de travail collectif.

543
00:43:53,960 --> 00:44:01,960
Et de son côté, quand j'ai demandé à Patrick Tresset quelle était la définition de la créativité,

544
00:44:01,960 --> 00:44:04,960
lui m'a dit que pour lui, c'était l'intention.

545
00:44:04,960 --> 00:44:12,960
C'est-à-dire qu'il parlait d'un dessin préhistorique, et ce dessin préhistorique faisait un zigzag.

546
00:44:12,960 --> 00:44:21,960
Et il me disait que le zigzag, ça ne peut pas être un dessin qui va être fait par erreur par un humain.

547
00:44:21,960 --> 00:44:27,960
Ça ne peut pas être juste un trait. J'ai un stylo dans la main et je fais un trait sur le mur à côté sans faire exprès.

548
00:44:27,960 --> 00:44:30,960
Le zigzag, ça marque forcément une intention.

549
00:44:30,960 --> 00:44:33,960
Et donc pour lui, c'est ça. C'est l'intention.

550
00:44:33,960 --> 00:44:42,960
Et donc là, on a affaire à deux définitions qui sont culturellement marquées et qui montrent bien que la créativité, c'est un objet anthropologique.

551
00:44:42,960 --> 00:44:47,960
C'est quelque chose qui va varier en fonction du contexte culturel dans lequel on évolue.

552
00:44:47,960 --> 00:44:51,960
Et dans les deux cas, l'intelligence artificielle aura servi à le révéler.

553
00:44:52,960 --> 00:44:55,960
Voilà, je suis désolée, j'ai été un peu longue.

554
00:44:55,960 --> 00:44:57,960
Non, non, mais c'était très bien.

555
00:44:57,960 --> 00:45:03,960
Il y a un truc, je vais rebondir sur un truc, mais avant ça, je voudrais dire salut à Agathe.

556
00:45:03,960 --> 00:45:06,960
Bienvenue à l'été dans un autre space que vous connaissez.

557
00:45:06,960 --> 00:45:08,960
C'est Agathe qui m'a donné.

558
00:45:08,960 --> 00:45:09,960
Coucou.

559
00:45:10,960 --> 00:45:12,960
Je vais te faire monter si tu veux, Agathe.

560
00:45:12,960 --> 00:45:15,960
Et puis salut aux autres aussi, évidemment.

561
00:45:15,960 --> 00:45:20,960
Donc, si vous voulez, si vous voulez participer à la discussion, n'hésitez pas à me demander le rôle de speaker.

562
00:45:20,960 --> 00:45:22,960
Et qu'est ce que je veux dire?

563
00:45:22,960 --> 00:45:23,960
Oui, alors, c'est ça.

564
00:45:23,960 --> 00:45:34,960
Tu parlais tout à l'heure du robot qui fait des œuvres et que l'artiste jouait un petit peu de ce truc là, de l'étonnement.

565
00:45:35,960 --> 00:45:36,960
Comment dire? Je ne sais pas.

566
00:45:36,960 --> 00:45:45,960
Je ne sais pas quel mot utiliser pour ça, mais le fait que les gens étaient assez impressionnés par le fait qu'il y ait une espèce de forme.

567
00:45:48,960 --> 00:45:53,960
Se déplacent et agissent d'elles-mêmes pour produire quelque chose, etc.

568
00:45:53,960 --> 00:45:56,960
Et j'imagine que ça doit être assez impressionnant.

569
00:45:56,960 --> 00:45:59,960
Et justement, j'ai pensé à ça.

570
00:45:59,960 --> 00:46:09,960
Et pas plus tard qu'aujourd'hui, quand j'ai vu une publicité pour une moto avec Full Self Driving, alors annoncé, je ne sais pas si c'est vrai.

571
00:46:09,960 --> 00:46:12,960
En effet, mais genre, en tout cas, c'était annoncé comme tel.

572
00:46:12,960 --> 00:46:15,960
Et la moto ressemblait énormément à la moto d'Akira.

573
00:46:15,960 --> 00:46:18,960
Donc, je me suis dit, j'ai tellement envie d'avoir un truc comme ça.

574
00:46:18,960 --> 00:46:19,960
Mais le truc, c'est que c'est ça.

575
00:46:19,960 --> 00:46:28,960
Je me suis dit, je regardais en fait comment il faisait, comment il faisait, comment il déplaçait la machine, comment il la guidait avec la voix, les gestes et tout.

576
00:46:28,960 --> 00:46:39,960
Et je me suis dit, ça, c'est drôle parce qu'à un moment donné, on va avoir un rapport avec notre véhicule qui va quasiment être un rapport d'humain à humain.

577
00:46:39,960 --> 00:46:43,960
Déjà que ça existe déjà un peu. Il y a des gens qui sont amoureux de leur camion, etc.

578
00:46:43,960 --> 00:46:53,960
Mais là, ça va être encore pire parce que l'objet va t'écouter, va te répondre, va interagir avec toi d'une manière qui sera de plus en plus organique.

579
00:46:54,960 --> 00:46:59,960
Et ça nous ramène à des films comme Heure, etc.

580
00:46:59,960 --> 00:47:06,960
On est en plein dedans, ce délire de l'émotion, en fait, puisqu'on va développer de l'émotion totalement pour des machines.

581
00:47:06,960 --> 00:47:20,960
Oui, c'est super intéressant parce que là, en fait, Patrick Trisset, il a joué avec sa machine pour qu'elle ait l'air la plus humaine possible.

582
00:47:20,960 --> 00:47:31,960
Par exemple, elle prend des photos des personnes qu'elle dessine, en fait, et elle va baisser la tête, puis lever la tête, regarder la personne, puis rebaisser la tête.

583
00:47:31,960 --> 00:47:39,960
Alors qu'en réalité, elle a besoin de prendre qu'une seule photo et après, elle peut dessiner complètement le portrait parce qu'elle a un code qui lui permet de dessiner.

584
00:47:39,960 --> 00:47:47,960
Mais en fait, c'est cette mimique de baisser la tête, lever la tête, prendre son temps pour dessiner qui va la rendre humaine.

585
00:47:47,960 --> 00:47:54,960
Elle mime l'imperfection pour que le public se dise qu'elle dessine comme un humain.

586
00:47:54,960 --> 00:48:04,960
Et il y a une autre œuvre qui montre bien à peu près la subjectivité qu'on a avec nos machines, à quel point on est proche.

587
00:48:04,960 --> 00:48:10,960
Il y a une œuvre que j'aime beaucoup d'une artiste qui s'appelle Lorraine McCarty.

588
00:48:10,960 --> 00:48:14,960
Elle est basée à LA et l'œuvre s'appelle Someone.

589
00:48:14,960 --> 00:48:18,960
Et en fait, elle imagine une version humaine d'Alexa.

590
00:48:18,960 --> 00:48:29,960
Donc, en fait, pendant deux mois, en 2019, il y a quatre personnes qui se sont installées dans une pièce et qui ont remplacé Alexa.

591
00:48:29,960 --> 00:48:43,960
Et donc, des volontaires, des personnes chez qui on a installé un programme, utilisaient cet Alexa humain en ayant conscience que c'était un humain ou une humaine derrière le programme.

592
00:48:43,960 --> 00:48:46,960
Et pas juste une IA.

593
00:48:46,960 --> 00:48:52,960
Donc, ils demandaient d'allumer ou de baisser la lumière ou d'allumer ou de baisser la musique.

594
00:48:52,960 --> 00:48:53,960
Et donc, ça change.

595
00:48:53,960 --> 00:49:03,960
Ça permettait de réfléchir quand même à la place qu'on donne à ces agents conversationnels dans nos vies.

596
00:49:03,960 --> 00:49:07,960
Et déjà, à la forme d'intimité qu'on a avec elles quand même.

597
00:49:07,960 --> 00:49:15,960
Après, effectivement, la question sur la voiture, par exemple, comment on s'attache à son camion, etc.

598
00:49:16,960 --> 00:49:18,960
C'est tout à fait ça.

599
00:49:18,960 --> 00:49:31,960
C'est tout à fait lié au mode d'existence que je mentionnais un peu plus tôt, qui sont un outil philosophique pour montrer que la technique existe de plusieurs façons.

600
00:49:31,960 --> 00:49:35,960
Elle peut exister comme un outil, mais elle peut aussi exister dans notre intimité.

601
00:49:35,960 --> 00:49:38,960
Elle peut aussi exister dans nos émotions, etc.

602
00:49:39,960 --> 00:49:49,960
Et ça, par exemple, il y a eu un livre de Bruno Latour au début, avant qu'il soit...

603
00:49:49,960 --> 00:49:54,960
Il était connu déjà, mais c'était un peu plus tôt dans sa carrière, ce qui s'appelait Aramis.

604
00:49:54,960 --> 00:49:58,960
Et c'était sur un métro parisien qui n'a jamais vu le jour.

605
00:49:58,960 --> 00:50:01,960
C'était un projet de métro.

606
00:50:01,960 --> 00:50:08,960
Et en fait, il allait faire une étude anthropologique du projet de métro.

607
00:50:08,960 --> 00:50:19,960
Et il a montré quels étaient les rapports à la technique, les différents rapports à la technique dans ce projet de métro, des personnes qui étaient impliquées dedans.

608
00:50:19,960 --> 00:50:23,960
Donc, il n'y a pas uniquement l'intelligence artificielle.

609
00:50:23,960 --> 00:50:28,960
Il y a des gens qui vont être très attachés à des objets techniques qui ne sont pas du tout intelligents.

610
00:50:28,960 --> 00:50:33,960
Même les poupées artificielles, je crois qu'on en a parlé un tout petit peu plus tôt.

611
00:50:33,960 --> 00:50:36,960
La majorité des poupées ne sont pas intelligentes.

612
00:50:36,960 --> 00:50:47,960
Ce sont des poupées qui n'ont pas du tout de bottes intégrées en elles, par exemple.

613
00:50:47,960 --> 00:50:52,960
Et pourtant, ça suffit pour qu'il y ait toute une population.

614
00:50:52,960 --> 00:50:56,960
C'est plutôt fréquent, plus au Japon qu'en France.

615
00:50:56,960 --> 00:51:02,960
Après le travail d'une anthropologue qui s'appelle Agnès Jarre, ça suffit pour que des gens leur prêtent une subjectivité.

616
00:51:02,960 --> 00:51:18,960
Vous allez avoir des gens qui vont avoir des poupées chez elles, et qui vont les habiller, et qui vont leur parler, et avoir un quotidien avec ces personnes-là, avec ces poupées-là, et leur prêter une subjectivité.

617
00:51:19,960 --> 00:51:24,960
Ça, en fait, ce n'est pas uniquement lié à l'intelligence artificielle.

618
00:51:24,960 --> 00:51:28,960
Il y a plein d'objets à qui on prête une subjectivité au quotidien.

619
00:51:28,960 --> 00:51:33,960
Dans notre culture, ça se fait de moins en moins, mais ça se fait aussi dans beaucoup de cultures.

620
00:51:33,960 --> 00:51:39,960
Et il y a un anthropologue qui s'appelle Denis Vidal, qui a écrit un livre qui s'appelle « Aux frontières de l'humain »,

621
00:51:40,960 --> 00:51:50,960
qui réfléchit à cette question-là, et il compare les IA et les robots à ces terrains anthropologiques.

622
00:51:50,960 --> 00:51:55,960
Lui, il a travaillé sur des poupées en Inde, et l'utilisation de ces poupées dans des rituels,

623
00:51:55,960 --> 00:52:01,960
et comment on prêtait une personnalité, une subjectivité à ces poupées.

624
00:52:01,960 --> 00:52:07,960
Et il compare ça avec le rapport qu'on peut entretenir avec un robot ou avec une IA.

625
00:52:07,960 --> 00:52:13,960
Et en fait, cette question de prêter une âme à un objet,

626
00:52:13,960 --> 00:52:19,960
elle a aussi été traitée par d'autres anthropologues qui sont proches de Denis Vidal,

627
00:52:19,960 --> 00:52:26,960
Emmanuel Grimaud, Philippe Descola aussi, Anne-Christine Taylor,

628
00:52:26,960 --> 00:52:30,960
dans une exposition qui a eu lieu au Quai Branly.

629
00:52:30,960 --> 00:52:34,960
Si ça vous intéresse, je vous conseille d'aller voir le catalogue d'exposition,

630
00:52:34,960 --> 00:52:36,960
qui s'appelait Persona, étrangement humain.

631
00:52:36,960 --> 00:52:42,960
Pourquoi ça s'appelait Persona? Parce que la Persona, c'est l'âme, en fait.

632
00:52:42,960 --> 00:52:45,960
C'est la conscience qu'on va prêter à quelqu'un.

633
00:52:45,960 --> 00:52:55,960
Et sur cette expo, ça rassemblait une série d'objets auxquels on a prêté une subjectivité.

634
00:52:56,960 --> 00:53:01,960
Ce sont des objets qui ne sont pas forcément des IA,

635
00:53:01,960 --> 00:53:06,960
qui ne trompent pas l'humain de façon...

636
00:53:06,960 --> 00:53:09,960
par les technologies dont on parle aujourd'hui,

637
00:53:09,960 --> 00:53:13,960
mais par d'autres façons, qui ne le trompent pas forcément d'ailleurs,

638
00:53:13,960 --> 00:53:18,960
parce qu'il y a aussi leur choix de croire que cet objet a une subjectivité.

639
00:53:18,960 --> 00:53:24,960
Donc là, on parle presque du domaine du religieux et de la croyance.

640
00:53:24,960 --> 00:53:26,960
C'est pour ça que je parlais d'ontologie un peu plus tôt,

641
00:53:26,960 --> 00:53:31,960
parce que par exemple, l'animisme japonais, c'est très difficile à définir,

642
00:53:31,960 --> 00:53:33,960
et ce n'est pas toujours le meilleur outil.

643
00:53:33,960 --> 00:53:38,960
Mais pour rester simple, et je dis ça très grossièrement,

644
00:53:38,960 --> 00:53:45,960
c'est le fait de prêter une âme aux autres objets, aux autres animaux, aux autres végétaux.

645
00:53:45,960 --> 00:53:52,960
C'est un excellent exemple d'une autre façon de vivre,

646
00:53:52,960 --> 00:53:54,960
et d'une autre façon de voir le monde,

647
00:53:54,960 --> 00:54:02,960
qui est beaucoup plus propice à une relation plus fusionnelle avec l'IA.

648
00:54:02,960 --> 00:54:06,960
Justement, plus fusionnelle, mais aussi beaucoup plus dangereuse.

649
00:54:06,960 --> 00:54:09,960
Je pense qu'on va avoir d'énormes problèmes à cause de ce truc-là dans le futur.

650
00:54:09,960 --> 00:54:13,960
Je vais passer la parole à Agathe.

651
00:54:13,960 --> 00:54:14,960
Salut Agathe, ça va?

652
00:54:14,960 --> 00:54:15,960
Je suis désolée, parce que je passe deux secondes.

653
00:54:15,960 --> 00:54:18,960
Désolée Sobe, mais je passe deux secondes, je dois partir.

654
00:54:18,960 --> 00:54:20,960
Du coup, je vais être très, très rapide.

655
00:54:20,960 --> 00:54:21,960
Coucou Carla.

656
00:54:21,960 --> 00:54:22,960
Coucou.

657
00:54:22,960 --> 00:54:25,960
Vraiment contente de voir que ça a eu lieu.

658
00:54:25,960 --> 00:54:28,960
Tu es venue sur Radio Tchad.

659
00:54:28,960 --> 00:54:29,960
Merci.

660
00:54:29,960 --> 00:54:31,960
Ce qu'on te dit est hyper intéressant.

661
00:54:31,960 --> 00:54:34,960
J'avais juste une petite question, parce que là, tu parlais d'une exposition.

662
00:54:34,960 --> 00:54:40,960
Et là, j'ai vu qu'il y avait jusqu'au début janvier, une expo à Paris.

663
00:54:40,960 --> 00:54:48,960
Je ne sais plus trop où c'est, mais sur les identités, j'ai l'impression que ça surfe un peu aussi avec cette idée du fait qu'avec le numérique,

664
00:54:48,960 --> 00:54:58,960
on a plein de personnes différentes, de personnes, personnages, identités, façons de se représenter.

665
00:54:58,960 --> 00:55:03,960
Et que finalement, ça pourrait interroger un peu aussi notre rapport à l'IA.

666
00:55:03,960 --> 00:55:06,960
Est-ce que tu l'as vue et est-ce que tu la recommandes ou pas?

667
00:55:06,960 --> 00:55:11,960
Parce que moi, j'ai vraiment bien envie d'y aller, mais si tu me dis que c'est nul, je n'y vais pas.

668
00:55:11,960 --> 00:55:13,960
Alors, je ne l'ai pas vue du tout.

669
00:55:13,960 --> 00:55:18,960
Et là, avec une recherche rapide, je suis seulement tombée sur un spectacle.

670
00:55:18,960 --> 00:55:23,960
Avec un gros bonhomme qui est très joli.

671
00:55:23,960 --> 00:55:25,960
Identité musicale.

672
00:55:25,960 --> 00:55:26,960
Identité.

673
00:55:26,960 --> 00:55:30,960
Ah, j'ai tapé IA, c'est pour ça que ça sort par exposition.

674
00:55:30,960 --> 00:55:32,960
Déjà, où est-ce que c'est?

675
00:55:32,960 --> 00:55:34,960
C'est à Paris.

676
00:55:34,960 --> 00:55:36,960
Identité.

677
00:55:36,960 --> 00:55:38,960
Que est un autre?

678
00:55:38,960 --> 00:55:42,960
Ah oui, c'est la Biennale d'art contemporain.

679
00:55:42,960 --> 00:55:46,960
Alors oui, c'est la Biennale NEMO, c'est la Biennale des arts numériques.

680
00:55:46,960 --> 00:55:52,960
Et donc, c'est l'exposition tous les deux ans, c'est au 104.

681
00:55:52,960 --> 00:55:53,960
Oui.

682
00:55:53,960 --> 00:55:55,960
Nos personnages.

683
00:55:59,960 --> 00:56:02,960
On a été musique et puis on a vu l'art numérique.

684
00:56:03,960 --> 00:56:05,960
Oui, c'est ça.

685
00:56:05,960 --> 00:56:08,960
Et donc, moi je n'y suis pas encore allée.

686
00:56:08,960 --> 00:56:11,960
Mais je sais que c'est une biennale.

687
00:56:11,960 --> 00:56:14,960
Et donc, j'ai suivi les deux dernières biennales.

688
00:56:14,960 --> 00:56:22,960
Et celle-ci, c'est le troisième volet d'un cycle qui s'est déroulé en trois biennales.

689
00:56:22,960 --> 00:56:24,960
Donc en six ans.

690
00:56:24,960 --> 00:56:31,960
Et donc, le premier volet s'intéressait à l'archéologie du numérique.

691
00:56:31,960 --> 00:56:40,960
Donc en fait, c'est-à-dire aux usages du numérique et des questions liées à l'intelligence artificielle dans le temps.

692
00:56:40,960 --> 00:56:44,960
Le deuxième volet, je crois, était plus axé sur le présent.

693
00:56:44,960 --> 00:56:51,960
Et là, ils essaient de conclure un peu, notamment avec la question des identités.

694
00:56:51,960 --> 00:56:59,960
Et donc, moi les deux éditions précédentes de la biennale, je les avais trouvées absolument formidables.

695
00:56:59,960 --> 00:57:05,960
Donc, je vais aller à ce troisième volet.

696
00:57:05,960 --> 00:57:08,960
Et je pense que je ne serai pas déçue non plus.

697
00:57:08,960 --> 00:57:15,960
Parce qu'en général, les artistes qui étaient exposés à la biennale NEMO étaient quand même excellents, très intéressants.

698
00:57:15,960 --> 00:57:20,960
Et beaucoup d'entre eux font partie de mon corpus d'œuvres d'art.

699
00:57:20,960 --> 00:57:30,960
Donc oui, en général, le 104, c'est quand même un très beau lieu qui croise plein de médiums différents.

700
00:57:30,960 --> 00:57:34,960
En plus, il y a souvent plein de danseurs dans l'entrée.

701
00:57:34,960 --> 00:57:41,960
Je trouve ça formidable qu'on lit la danse avec les arts plastiques déjà dans un seul espace.

702
00:57:41,960 --> 00:57:45,960
Et ensuite, je trouve que le commissariat d'exposition est très bon.

703
00:57:45,960 --> 00:57:49,960
Les œuvres sélectionnées sont toujours très intéressantes.

704
00:57:49,960 --> 00:57:52,960
Les cartels sont bien développés.

705
00:57:52,960 --> 00:58:01,960
On comprend bien le sujet des œuvres, sans non plus nous perdre dans un flot d'informations complètement flou.

706
00:58:01,960 --> 00:58:04,960
Parce que c'est des fois difficile avec l'art contemporain.

707
00:58:04,960 --> 00:58:06,960
Donc, je ne l'ai pas encore faite.

708
00:58:06,960 --> 00:58:11,960
Mais je pense que je peux raisonnablement dire qu'en général, c'est du bon travail.

709
00:58:11,960 --> 00:58:13,960
Ok, écoute, merci beaucoup. Je pense que je vais y aller.

710
00:58:13,960 --> 00:58:20,960
Moi, c'est un sujet, surtout dans les cryptos, où je trouve qu'on se présente sur plein d'identités différentes,

711
00:58:20,960 --> 00:58:24,960
qui me parlent beaucoup avec le pseudonyme, etc.

712
00:58:24,960 --> 00:58:29,960
Et puis, l'entrée fracassante de l'IA dans nos vies, le justifie aussi.

713
00:58:29,960 --> 00:58:33,960
Donc, je suis désolée, j'aurais adoré discuter plus avec vous, mais je dois vraiment y aller.

714
00:58:33,960 --> 00:58:38,960
Merci, Radio Tchad. J'irais écouter les replays aussi, parce que j'ai raté tout le début.

715
00:58:38,960 --> 00:58:41,960
Et désolée Sobe pour ta voix bypass.

716
00:58:41,960 --> 00:58:43,960
Merci Carla. Merci à toi.

717
00:58:43,960 --> 00:58:44,960
Merci d'être venue.

718
00:58:44,960 --> 00:58:45,960
Ciao.

719
00:58:45,960 --> 00:58:46,960
Ciao.

720
00:58:48,960 --> 00:58:51,960
Et du coup, Sobe, ça va?

721
00:58:51,960 --> 00:58:54,960
Oui, bien sûr. D'ailleurs, remerciement. Merci.

722
00:58:54,960 --> 00:58:59,960
J'apprécie que l'intervenante m'ait citée.

723
00:58:59,960 --> 00:59:03,960
Bon, non, mais elle ne m'a pas non plus coupé la parole.

724
00:59:03,960 --> 00:59:08,960
Mais c'est agréable. Merci pour ta politesse.

725
00:59:09,960 --> 00:59:13,960
Qu'est-ce que je voulais dire? Je ne sais pas. Je suis montée parce que je trouvais ça très intéressant.

726
00:59:13,960 --> 00:59:16,960
Alors, moi, je ne suis pas de votre génération, manifestement.

727
00:59:16,960 --> 00:59:20,960
Je le remarque par vos interventions, mais c'est très intéressant.

728
00:59:20,960 --> 00:59:23,960
Et c'est le sujet qui est très intéressant.

729
00:59:23,960 --> 00:59:29,960
C'est l'art digital versus l'art traditionnel, en somme, si j'ai bien compris.

730
00:59:29,960 --> 00:59:31,960
Un peu.

731
00:59:31,960 --> 00:59:38,960
Alors, oui, ce n'est pas un versus dans le sens où l'un peut tout à fait être en harmonie avec l'autre.

732
00:59:38,960 --> 00:59:39,960
Mais oui, c'est ça.

733
00:59:39,960 --> 00:59:45,960
En fait, c'est ça, c'est de voir s'il y a vraiment un versus.

734
00:59:45,960 --> 00:59:49,960
Ou si on est dans une continuité ou pas.

735
00:59:49,960 --> 00:59:51,960
Oui, d'accord. Merci.

736
00:59:51,960 --> 00:59:54,960
Merci. Moi, je l'avais entendu comme ça.

737
00:59:54,960 --> 00:59:56,960
Juste le titre, simplement le titre.

738
00:59:56,960 --> 00:59:57,960
Oui, oui, oui.

739
00:59:57,960 --> 00:59:59,960
D'accord. Alors, je n'ai évidemment pas la réponse.

740
00:59:59,960 --> 01:00:02,960
Bien entendu. Bien entendu.

741
01:00:02,960 --> 01:00:05,960
Je n'ai que des questions et en plus, je suis totalement novice.

742
01:00:05,960 --> 01:00:09,960
Alors, moi, je suis old school dans le sens où je suis plus âgé que vous.

743
01:00:09,960 --> 01:00:19,960
Donc, je pense que j'ai une moindre connaissance, surtout de l'art numérique.

744
01:00:19,960 --> 01:00:22,960
Après, le numérique, ça va. Je check.

745
01:00:22,960 --> 01:00:24,960
Je bosse avec des ordinateurs et tout.

746
01:00:24,960 --> 01:00:27,960
Mais surtout sur l'art numérique, j'en ai pas.

747
01:00:27,960 --> 01:00:31,960
Et je ne suis pas fermée à l'idée.

748
01:00:31,960 --> 01:00:33,960
Il n'y a pas d'opposition.

749
01:00:33,960 --> 01:00:39,960
Je me dis juste que l'art, enfin, en ce que moi, je ressens,

750
01:00:39,960 --> 01:00:46,960
ce que j'ai compris à la fois dans mon éducation traditionnelle,

751
01:00:46,960 --> 01:00:51,960
j'entends ma personne et dans ce que j'ai appris moi en cours.

752
01:00:51,960 --> 01:00:55,960
Après, je ne vais pas raconter ma vie, mais bon.

753
01:00:55,960 --> 01:00:57,960
Mais peu importe.

754
01:00:57,960 --> 01:01:04,960
Il y a forcément dans l'art, on a toujours, même dans l'art traditionnel,

755
01:01:04,960 --> 01:01:10,960
c'est celui que je connais dans l'histoire de l'art, tout bonnement, une évolution.

756
01:01:10,960 --> 01:01:13,960
Cette évolution, elle est marquée par la création.

757
01:01:13,960 --> 01:01:19,960
La création évolue.

758
01:01:20,960 --> 01:01:21,960
Voilà.

759
01:01:23,960 --> 01:01:28,960
J'arrive pas à synthétiser ma pensée.

760
01:01:28,960 --> 01:01:32,960
Ce que je veux dire par là, c'est que dans l'histoire de l'art,

761
01:01:32,960 --> 01:01:35,960
la création a évolué.

762
01:01:35,960 --> 01:01:39,960
Donc, ce que je voulais dire, c'est qu'elle évolue aujourd'hui.

763
01:01:39,960 --> 01:01:42,960
Elle évolue et elle évoluera avec l'IA.

764
01:01:42,960 --> 01:01:45,960
Ça, c'est une évidence.

765
01:01:45,960 --> 01:01:46,960
Voilà.

766
01:01:46,960 --> 01:01:50,960
Pour moi, il n'est pas question de dire pour ou contre.

767
01:01:50,960 --> 01:01:52,960
C'est comme ça. Point, ligne.

768
01:01:52,960 --> 01:01:54,960
Tout comme dans l'histoire de l'art.

769
01:01:54,960 --> 01:01:55,960
Il y a eu des événements.

770
01:01:55,960 --> 01:01:56,960
Non, oui, voilà.

771
01:01:56,960 --> 01:01:57,960
Peut-être, mais c'est mon opinion.

772
01:01:57,960 --> 01:01:58,960
Je parle pour moi.

773
01:01:58,960 --> 01:01:59,960
Voilà.

774
01:01:59,960 --> 01:02:04,960
Donc, juste, je ne veux pas t'interrompre, mais si jamais tu as une question ou quoi,

775
01:02:04,960 --> 01:02:07,960
est-ce que tu peux arriver au point?

776
01:02:07,960 --> 01:02:13,960
Parce qu'en gros, c'est parce que je crois que Carla doit partir depuis trois minutes déjà.

777
01:02:13,960 --> 01:02:16,960
Je ne sais pas à quel point tu es flexible.

778
01:02:16,960 --> 01:02:18,960
Je peux tout à fait écouter la question.

779
01:02:18,960 --> 01:02:19,960
D'accord.

780
01:02:19,960 --> 01:02:26,960
J'écoute la question et puis c'est pas pressé, pressé, mais oui, il va falloir que je file rapidement.

781
01:02:26,960 --> 01:02:28,960
C'est quoi ta dernière limite?

782
01:02:28,960 --> 01:02:31,960
La plus grosse?

783
01:02:31,960 --> 01:02:33,960
C'est quoi le plus tard?

784
01:02:33,960 --> 01:02:36,960
Ce serait le plus tôt possible aussi.

785
01:02:36,960 --> 01:02:37,960
D'accord.

786
01:02:37,960 --> 01:02:38,960
Merci.

787
01:02:38,960 --> 01:02:41,960
Je peux répondre à la question.

788
01:02:41,960 --> 01:02:48,960
J'attends juste d'entendre la fin du propos et je répondrai avec plaisir.

789
01:02:48,960 --> 01:02:50,960
S'il y a une question, si ça se trouve, c'est juste un commentaire.

790
01:02:50,960 --> 01:02:52,960
Absolument, il n'y avait pas de question.

791
01:02:52,960 --> 01:02:54,960
C'était juste un commentaire.

792
01:02:54,960 --> 01:02:57,960
Oui, c'est tout à fait vrai.

793
01:02:57,960 --> 01:03:07,960
En plus, l'histoire de l'art, c'est une série de jalons qui remettent en cause des acquis.

794
01:03:07,960 --> 01:03:09,960
Oui, qui évoluent.

795
01:03:09,960 --> 01:03:12,960
C'est ça qui est passionnant avec l'histoire de l'art.

796
01:03:12,960 --> 01:03:17,960
Il y a des moments de querelles entre les modernes et les classiques.

797
01:03:17,960 --> 01:03:22,960
Et à ce moment-là, ce ne sont pas toujours les modernes qui gagnent.

798
01:03:22,960 --> 01:03:26,960
Donc, je pense qu'on est encore dans une période…

799
01:03:26,960 --> 01:03:28,960
Il n'y a pas nécessairement de querelles.

800
01:03:28,960 --> 01:03:32,960
Oui, mais c'est juste qu'on parle historiquement souvent de querelles.

801
01:03:32,960 --> 01:03:36,960
Là, oui, est-ce que c'est une querelle ou pas?

802
01:03:36,960 --> 01:03:38,960
Non, je ne pense pas.

803
01:03:38,960 --> 01:03:39,960
Moi non plus.

804
01:03:39,960 --> 01:03:40,960
Ça dépend.

805
01:03:40,960 --> 01:03:41,960
Je pense qu'il n'y en a pas.

806
01:03:41,960 --> 01:03:43,960
Il y en a peut-être pas.

807
01:03:43,960 --> 01:03:46,960
Il y en a peut-être sur certains terrains et pas sur d'autres,

808
01:03:46,960 --> 01:03:49,960
parce que c'est un sujet qui est très large.

809
01:03:49,960 --> 01:03:52,960
Mais il y a des débats, en tout cas, c'est certain.